---
title: "Transit Cost Analysis"
subtitle: "Tidy Tuesday, 2021-01-05"
author: "Juha Päällysaho"
date: "2021-04-01"
date-modified: "2023-11-23"
execute:
  echo: true
  warning: false
draft: true
format: html
---

# prep
## libs

```{r}
library(janitor)
library(skimr)
library(correlationfunnel)
library(DataExplorer)
library(gghighlight)
library(patchwork)
library(tidyverse)
```


## get the data

```{r}
tuesdata <- tidytuesdayR::tt_load("2021-01-05")

transit_cost <- tuesdata |> pluck(1)
```



# eda

## what explains the price

```{r}
transit_cost <- transit_cost %>%
  filter((!is.na(e) | rr != 1), real_cost > 0, country != "FI") %>%
  type_convert(na = c("years", "5 NA"))
```


```{r}
transit_cost %>%
  count(real_cost, sort = T)

transit_cost %>%
  skim()
```

```{r}
transit_cost %>%
  ggplot(aes(tunnel, cost_km_millions)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm") +
  labs(title = "Tunnel Prices", x = "tunnel lenght", y = "costs per kilometer")
```

```{r}
library(countrycode)

transit_cost <- transit_cost %>%
  mutate(continent = countrycode(country, "iso2c", "continent"))


transit_cost %>%
  ggplot(aes(cost_km_millions, length)) +
  geom_point() +
  facet_wrap(vars(continent))
```
clear cluster, but americans, asia and europe has outliers:


```{r}
library(gghighlight)


transit_cost %>%
  ggplot(aes(cost_km_millions, length)) +
  geom_point() +
  gghighlight(cost_km_millions > 1000, label_key = line) +
  facet_wrap(vars(continent))
```


```{r}
transit_cost %>%
  ggplot(aes(cost_km_millions, length)) +
  geom_point() +
  # gghighlight(length > 5000 , label_key = line ) +


  facet_wrap(vars(continent))
```


less data:

```{r}
transit_cost %>%
  # filter(str_length(start_year) > 4) %>%
  count(start_year, sort = T)

less <- transit_cost %>%
  select(line, length, tunnel, stations, real_cost, continent) %>%
  filter(!is.na(continent))
```

## missing values

```{r}
less %>% plot_missing()
```
## tunnels

```{r}
less %>%
  filter(is.na(tunnel))

less %>%
  filter(!is.na(tunnel)) %>%
  group_by() %>%
  summarise(min(tunnel), max(tunnel), mean(tunnel), median(tunnel))

less %>%
  ggplot(aes(tunnel)) +
  geom_histogram()
```

## feature analysis

### categorical variables:

```{r}
less %>% plot_bar(nrow = 4)
```

### correlatations

```{r}
bina <- less %>%
  drop_na() %>%
  binarize(n_bins = 5)

bina %>%
  glimpse()



bina %>%
  correlate(`real_cost__6274.292_Inf`) %>%
  plot_correlation_funnel()
```
### numerical values

```{r}
less %>% plot_histogram()
```

data have high skew.


```{r}
less %>%
  ggplot(aes(length, real_cost)) +
  geom_violin() +
  geom_jitter(alpha = 0.5) +
  scale_y_log10()
```

# modeling

```{r}
library(tidymodels)
library(xgboost)
```


```{r}
set.seed(123)
initial_split <- initial_split(less, prop = 0.80)

trn <- training(initial_split)

tst <- testing(initial_split)
```

model recipe:

TODO: why these things are selected to the model

```{r}
preprocessing_recipe <- recipe(real_cost ~ length + stations + tunnel + continent, data = trn) %>%
  # Encode Categorical Data Types
  # step_dummy(all_nominal()) %>%

  # Combine low-frequency categories
  # step_other(all_nominal(), threshold = 0.02, other = "other") %>%

  # Impute missing
  step_impute_knn(all_predictors(), neighbors = 3) |>
  prep()

# Remove unnecessary columns
# step_rm(creation_date, line) %>
```

```{r}
transit_baked <- preprocessing_recipe %>%
  bake(new_data = NULL)
```

# validate

TODO: why this is done?

categorical features:

```{r}
transit_baked %>% plot_bar(maxcat = 5)
```


numerical:

```{r}
transit_baked %>%
  plot_histogram()
```

# cross validation

TODO: why cross validation is needed?

```{r}
set.seed(123)

car_cv_folds <- trn %>%
  vfold_cv(v = 5)

car_cv_folds
```

# models 

## regression


```{r}
glmnet_model <- linear_reg(
  mode = "regression"
) %>%
  set_engine("lm")

glmnet_model
```

## xgbooost

```{r}
xgboost_model <- boost_tree(
  mode       = "regression",
  trees      = 1000,
  min_n      = tune(),
  tree_depth = tune(),
  learn_rate = tune()
) %>%
  set_engine("xgboost", objective = "reg:squarederror")

xgboost_model
```

```{r}
glmnet_params <- parameters(penalty(), mixture())
glmnet_params

set.seed(123)
glmnet_grid <- grid_max_entropy(glmnet_params, size = 20)
glmnet_grid
```



```{r}
glmnet_grid %>%
  ggplot(aes(penalty, mixture)) +
  geom_point(size = 3) +
  scale_x_log10() +
  labs(title = "Max Entropy Grid", x = "Penalty (log scale)", y = "Mixture")
```

```{r}
xgboost_params <- parameters(min_n(), tree_depth(), learn_rate())
xgboost_params

set.seed(123)
xgboost_grid <- grid_max_entropy(xgboost_params, size = 30)
xgboost_grid
```

```{r}
glmnet_stage_1_cv_results_tbl <- tune_grid(
  object = glmnet_model,
  preprocessor = preprocessing_recipe,
  resamples = car_cv_folds,
  grid = glmnet_grid,
  metrics = metric_set(mae, mape, rmse, rsq),
  control = control_grid(verbose = TRUE)
)
```



```{r}
xgboost_stage_1_cv_results_tbl <- tune_grid(
  object = xgboost_model,
  preprocessor = preprocessing_recipe,
  resamples = car_cv_folds,
  grid = xgboost_grid,
  metrics = metric_set(mae, mape, rmse, rsq),
  control = control_grid(verbose = TRUE)
)
```

TODO: select best model / conclusions
